\chapter{Methodology}

This paper is based on the ideas of the AlphaZero framework applied to the game of Damath. Damath is a two-player board game combining the Filipino Checkers ``Dama'' and Mathematics. Each piece has a corresponding number and every  white square on the board has a mathematical operation. The researchers implemented the AlphaZero framework as a cycle of three stages. The researchers first generates data through self-play using the modified Monte-Carlo Tree Search algorithm of AlphaZero. Afterwards, the model is trained on the data generated from self-play. Finally, the current version of the model is evaluated against the previous model through competition.

\section{Neural Network}

The Neural Network $f_{\theta}$ in the original AlphaZero paper \cite{silver2017masteringchessshogiselfplay} takes as input a state $s$ and outputs a tuple $f_{\theta}(s) = (p, v)$, where $p$ is a probability distribution over possible actions (the policy vector) and $v \in [-1, 1]$ is a scalar value prediction estimating the expected outcome from state $s$.


\subsection{Architecture}

In our modification, inspired by (\cite{czech2024representationmattersmasteringchess}), we replace the scalar value with a three-dimensional win-draw-loss (WDL) vector $v_{\text{WDL}} = (p_w, p_d, p_l)$, where each component represents the probability that the current position will result in a win, draw, or loss respectively, with $p_w + p_d + p_l = 1$. This representation provides the model with richer outcome information, allowing it to explicitly differentiate between winning, drawing, and losing probabilities rather than collapsing this information into a single scalar value. The MCTS implementation still requires a scalar value for backpropagation which is computed by $v = p_w - p_l$.

Recent developments in the field of computer vision have shown that transformers are capable of beating Convolutional Neural Networks (\cite{dosovitskiy2021imageworth16x16words}) this is why we replaced the CNNs with the transformer architecture (\cite{vaswani2023attentionneed}).

% TODO: Figure of the model

\subsection{State Representation}

The state of the game is encoded as a $32 \times 25$ matrix. Where each row in the matrix represents a placeable cell in the board. 

\begin{table}[htb]
  \centering
  \begin{tabular}{|p{2cm} | p{2cm} | p{10cm} |}
    \hline 
    Dimension & Type & Description \\ \hline
    0  &  boolean  & The color of the current player. \\ \hline
    1-13  & boolean &  One-hot encoding of the weight of the piece. \\ \hline
    14 & boolean & Whether the piece is knighted or not. \\ \hline
    15 & boolean & Whether the current player owns this piece. \\ \hline
    16 & float & The score of the current player. \\ \hline
    17 & float & The score of the the opposing player. \\ \hline
    18 & float & How close the game is to a forced ending. \\ \hline
    19-22 & boolean & One-hot encoding of the operators in the board . \\ \hline
    23 & boolean & Whether the current piece is forced to eat. \\ \hline
    24 & boolean & The previous cell of the piece which is forced to eat. \\ \hline
  \end{tabular}
\end{table}


% TODO: Figure of state representation

\subsection{Action Representation}

% TODO: Figure of action representation


\section{Model Generation}

The ultimate goal of the paper is to produce a model that can beat human players in the game of Damath, it has three phases the self-play data generation, model training and model evaluation. These three phases are repeated $n$ times, with each iteration producing a better model. The pseudocode below summarizes the entire process. 

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{learn}{}
          \State model $\gets$ initialize with random parameters
          \State best\_model $\gets$ \Call{clone}{model}
          \Repeat
            \State dataset $\gets$ \Call{self\_play}{best\_model}
            \State \Call{train}{model, dataset}
            \State wins, draws, losses $\gets$ \Call{evaluate}{model, best\_model}
            \If{$\text{wins} + \text{draws} \ge 0.7 \times \text{num\_evaluations}$} 
                \State best\_model $\gets$ \Call{clone}{model}
            \EndIf
            \Until{$n = \text{num\_iterations}$}
           \State \Return best\_model
        \EndFunction
    \end{algorithmic}
\end{algorithm}


\subsection{Self-Play Data Generation}

The dataset needed for training the model is produced by pitting the best model against itself. It uses the monte-carlo tree search algorithm which is discussed in detail below.

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{self\_play}{model}
            \State history $\gets$ \{\}
            \State $s$ $\gets$ game.initial\_state()
            \Loop
                \State $p$ $\gets$ \Call{search}{$s$, model}
                \State history $\gets$ history $\cup$ ($s$, $p$)
                \State action $\gets$ \Call{sample\_distribution}{$p$}
                \State $s'$ $\gets$ game.apply\_action($s$, action)
                \If{$s'$ is terminal}
                    \State memory $\gets$ \{\}
                    \ForAll{($s_i$, $p_i$) $\in$ history}
                        \State outcome $\gets$ game.get\_outcome($s'$)

                        \If{$s_i$ and $s$ have different players}
                          \State flip outcome
                        \EndIf

                        \If{outcome is win}
                          \State $v$ $\gets$ $[1, 0, 0]$
                        \ElsIf{outcome is lose}
                          \State $v$ $\gets$ $[0, 0, 1]$
                        \ElsIf{outcome is draw}
                          \State $v$ $\gets$ $[0, 1, 0]$
                        \EndIf


                        \State memory $\gets$ memory $\cup$ ($p_i$, $v$, $s_i$)
                    \EndFor
                    \State \Return memory
                \EndIf
                \State $s$ $\gets$ $s'$
            \EndLoop
        \EndFunction
    \end{algorithmic}
    \caption{Pseudocode for the Self-Play Data Generation Phase of the AlphaZero Framework}
    \label{alg:data-generation}
\end{algorithm}

\subsubsection{Monte-Carlo Tree Search Algorithm}

The MCTS Algorithm guides the model in making actions. It has three phases, the selection, expansion, and backpropagation.

The selection phase selects the node with the highest score. The score function takes into account the perspective of the player who played the previous move. If the previous player is not equal to the current player we adjust the score to select the action that minimizes the score of the player otherwise it selects the action that maximizes the score. 

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{select}{$s$, node}
            \While{node.is\_expanded()}
                \State node $\gets$ \Call{select\_child}{node}
                \State $s$ $\gets$ game.apply\_action($s$, node.action)
            \EndWhile
            \State \Return $s$, node
        \EndFunction
        
        \Function{select\_child}{node}
            \State child\_scores $\gets$ \{\}
            \ForAll{child $\in$ node.children}
                \State child\_scores $\gets$ child\_scores $\cup$ \Call{score}{child}
            \EndFor
            \Return \Call{max}{child\_scores}
        \EndFunction
        
        \Function{score}{node}
            \If{node.visits $>$ 0}
                \State mean $\gets \dfrac{1}{2}\cdot\left(\dfrac{ \text{node.value} }{ \text{node.visits} } + 1\right)$
                \If{node.parent.player $\neq$ node.player}
                    \State mean $\gets$ 1 $-$ mean
                \EndIf
            \Else
                \State mean $\gets$ 0
            \EndIf
            \State \Return $\text{mean} + \text{node.prior} \cdot C \cdot \dfrac{\sqrt{\text{node.parent.visits}}}{1 + \text{node.visits}}$  
        \EndFunction
    \end{algorithmic}
    \caption{Select Function for the Monte-Carlo Tree Search Algorithm}
    \label{alg:select}
\end{algorithm}
\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{search}{$s$, model, simulations}
            \State root $\gets$ Node($s$)
            \Repeat
                \State node $\gets$ root
                \State $s'$, node $\gets$ \Call{select}{node}
                \If{$s'$ is terminal}
                    \If{$s$ and $s'$ have different players}
                      \State flip outcome
                    \EndIf
                    \If{outcome is win}
                      \State $v$ $\gets$ $1$
                    \ElsIf{outcome is draw}
                      \State $v$ $\gets$ $0$
                    \ElsIf{outcome is loss}
                      \State $v$ $\gets$ $-1$
                    \EndIf
                \Else
                    \State $v$ $\gets$ \Call{expand}{s, model}
                \EndIf
        
                \State \Call{backpropagate}{node, $v$}
            \Until{$n > \text{simulations}$}

            \State $p \gets \{\}$
            \ForAll{child $\in$ root.children}
            \State $p \gets p \cup \text{child.visits}$
            \EndFor
            \State \Return $p / \sum p$
        \EndFunction
    \end{algorithmic}
    \caption{Monte-Carlo Tree Search Algorithm}
    \label{alg:mcts}
\end{algorithm}

The algorithm repeats the selection phase until it encounters a node that is not expanded. There are two cases, it encounters terminal node or a non-terminal node. 

If it encounters a non-terminal node, then this signifies an action that has not been explored yet. The canonical MCTS uses a rollout phase that randomly selects an action from the legal actions, but similar to the alphazero paper we use the model to evaluate the possible actions and assign a probability to each of them. To restrict the model from assigning probabilities to invalid actions we obtain the legal actions from the game and filter the policy vector for only the legal actions.

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{expand}{$s$, model}
            \State $\text{legal\_actions} \gets \text{game.get\_legal\_actions(s)}$
            \State $p, v \gets \text{model(s)}$
            \State $p \gets \Call{softmax}{p}$
            \State $p \gets \Call{filter}{\text{legal\_actions}, p}$
            \State $p \gets p / \sum p$
        
            \For{(action, prior) $\in$ (legal\_actions, policy)}
                \State $s' \gets \text{game.apply\_action($s$, action)}$
                \State parent.children $\gets$ parent.children $\cup$ Node($s'$, action, prior)
            \EndFor
            \State $(p_w, p_d, p_l) \gets v$ 
            \State \Return $p_w - p_l$
        \EndFunction
    \end{algorithmic}
    \caption{Expand Function for the Monte-Carlo Tree Search Algorithm}
    \label{alg:expand}
\end{algorithm}

If it encounters a terminal node, then it backpropagates the outcome of the game which is a scalar value $\in [-1, 1]$ which is $1$ for win, $-1$ for lose and $0$ for draw. The backpropagation phase is essential for updating the statistics of the nodes that have been traveresed by the current iteration. Here, it updates the cumulative value of each node as well as their respective visit counts.

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{backpropagate}{node, value}
            \While{node $\neq$ null}
                \State node.visits += 1
                \If{node.parent.state.player == node.state.player} 
                    \State node.value += value
                \Else
                    \State node.value -= value
                \EndIf
                \State node $\gets$ node.parent
            \EndWhile
        \EndFunction
    \end{algorithmic}
    \caption{Backpropagrate Function for the Monte-Carlo Tree Search Algorithm}
    \label{alg:backpropagate}
\end{algorithm}


\subsection{Model Training}

After the self-play generation, the model is then trained using the data generated from the \ref{alg:data-generation}. The dataset is comprised of a tuples of the form $(\pi, v, s)$, where $\pi$ is the computed policy by MCTS, $v$ is a one-hot encoding of the outcome of the game, $[1, 0, 0]$ for a win, $[0, 1, 0]$ for a draw and $[0, 0, 1]$ for a loss and $s$ is the state of the game.

The loss $l$ is computed by the following equation,
\begin{align}
  p', v' &= f_{\theta}(s) \\
  l &= \sum_{}^{} v_{i}\log(v_{i}') + \sum_{}^{}p_{i}\log(\p_{i}')
  \label{eq:loss}
\end{align}

The computed loss is then used by the optimizer for optimizing the parameters of the model. We use the Adam Optimizer (\cite{kingma2017adammethodstochasticoptimization}) as the optimizer. 

\subsection{Model Evaluation}

After training, the newly trained model is pitted against the best model for 50 rounds of games. This is done by switching the model depending on the current player of the state $s$. If the player to move in $s$ is the first player, then the action probabilites $\pi$ is obtained from the first model, otherwise $\pi$ is obtained from the best model.

The wins, draws and losses of the current model is recorded. If the current model achieves $70\%$ of the wins and draws out of the total number of games played, then it becomes the best model. 

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{evaluate}{current\_model, best\_model}
          \State wins $\gets$ 0
          \State losses $\gets$ 0
          \State draws $\gets$ 0
          \Repeat
            \State $s \gets \text{game.initial\_state()}$ 
            \Loop

                \If{$s$ is made by the first player}
                  \State model $\gets$ current\_model
                \Else
                  \State model $\gets$ best\_model
                \EndIf

                \State $p$ $\gets$ \Call{search}{$s$, model}
                \State action $\gets$ \Call{sample\_distribution}{$p$}
                \State $s'$ $\gets$ game.apply\_action($s$, action)
                \If{$s'$ is terminal}
                    \State outcome $\gets$ game.get\_outcome($s'$)

                    \If{$s'$ is made by the second player}
                      \State flip outcome
                    \EndIf

                    \If{outcome is win}
                      \State wins $\gets$ wins + 1
                    \ElsIf{outcome is draw}
                      \State draws $\gets$ draws + 1
                    \ElsIf{outcome is loss}
                      \State loss $\gets$ loss + 1
                    \EndIf
                    \State \textbf{end loop}
                \EndIf
                \State $s$ $\gets$ $s'$
            \EndLoop
          \Until{$n < \text{num\_rounds}$}
          \State \Return wins, draws, losses
        \EndFunction
    \end{algorithmic}
    \label{alg:evaluate}
\end{algorithm}

\section{Model Assessment}

To assess the strength of the final model after all the iterations this model is pitted against the all the previous best models. We save the previous best models throughout all the iterations. The wins of the final model should trend downwards as it is pitted from the earliest to the latest previous best models.

% TODO: finalize the number of games to play 
We also employ the help of a National Damath Champion, to quantitively assess the strength of the model by playing a game of $8$ rounds against the best model. And also provide some analysis over the moves that the models made throughout the games.

