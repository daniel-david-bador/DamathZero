\chapter{Methodology}

This paper is based on the ideas of the AlphaZero framework applied to the game of Damath. Damath is a two-player board game combining the Filipino Checkers ``Dama'' and Mathematics. Each piece has a corresponding number and every  white square on the board has a mathematical operation. The researchers implemented the AlphaZero framework as a cycle of three stages. The researchers first generates data through self-play using the modified Monte-Carlo Tree Search algorithm of AlphaZero. Afterwards, the model is trained on the data generated from self-play. Finally, the current version of the model is evaluated against the previous model through competition.

\section{State Representation}

The state of the current game is represented as an $8 \times 8 \times 6$ tensor where each plane represent a feature of the current game. The first layer of the tensor is an $8 \times 8$ matrix filled with the current score of the current player while the second layer represents is filled with the score of the opposing player. The third layer is an $8 \times 8$ matrix filled with the values of the normal pieces of the current player on their corresponding position in the board of the current game state. The fourth layer is an $8 \times 8$ matrix filled with the values of the dama pieces of the current player on their corresponding position in the board of the current game state. The fifth layer is an $8 \times 8$ matrix filled with the values of the normal pieces of the opposing player on their corresponding position in the board of the current game state. The sixth layer is an $8 \times 8$ matrix filled with the values of the dama pieces of the current player on their corresponding position in the board of the current game state.

\section{Action Representation}

An action is represented as an index to the action space of Damath. The action space of Damath is represented as a $12 \times 4 \times 7$ tensor as shown in Figure \ref{fig:action_space}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\linewidth]{images/action_space.png}
    \caption{Action Space of Damath}
    \label{fig:action_space}
\end{figure}

The first dimension of the action space with a size of 12 represents one of the 12 playable pieces of the current player taking an action. The second dimension of the action space with a size of 4 represents one of the four directions on which the current piece is moving towards to. The four directions are upper left, upper right, lower left, and lower right. The last dimension of the action space represents the number of tiles the current piece is jumping over.

\section{Neural Network}

The Neural Network $f_{\theta}$ in the original AlphaZero paper \cite{silver2017masteringchessshogiselfplay} takes as input a state $s$ and outputs a tuple $f_{\theta}(s) = (p, v)$, where $p$ is a probability distribution over possible actions (the policy vector) and $v \in [-1, 1]$ is a scalar value prediction estimating the expected outcome from state $s$.

In our modification, inspired by \cite{czech2024representationmattersmasteringchess}, we replace the scalar value with a three-dimensional win-draw-loss (WDL) vector $v_{\text{WDL}} = (p_w, p_d, p_l)$, where each component represents the probability that the current position will result in a win, draw, or loss respectively, with $p_w + p_d + p_l = 1$. This representation provides the model with richer outcome information, allowing it to explicitly differentiate between winning, drawing, and losing probabilities rather than collapsing this information into a single scalar value. The MCTS implementation still requires a scalar value for backpropagation which is computed by $v = p_w - p_l$.

Recent developments in the field of computer vision have shown that transformers are capable of beating Convolutional Neural Networks (\cite{dosovitskiy2021imageworth16x16words}) this is why we replaced the CNNs with the transformer architecture \cite{vaswani2023attentionneed}.


\subsection{Monte-Carlo Tree Search Algorithm}

The MCTS Algorithm guides the model in making actions. It has three phases, the selection, expansion, and backpropagation.

The selection phase selects the node with the highest score. The score function takes into account the perspective of the player who played the previous move. If the previous player is not equal to the current player we adjust the score to select the action that minimizes the score of the player otherwise it selects the action that maximizes the score. 

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{select}{node}
            \While{node.is\_expanded()}
                \State node $\gets$ \Call{select\_child}{node}
            \EndWhile
        \EndFunction
        
        \Function{select\_child}{node}
            \State child\_scores $\gets$ \{\}
            \ForAll{child $\in$ node.children}
                \State child\_scores $\gets$ child\_scores $\cup$ \Call{score}{child}
            \EndFor
            \Return \Call{max}{child\_scores}
        \EndFunction
        
        \Function{score}{node}
            \If{node.visits $>$ 0}
                \State mean $\gets \dfrac{1}{2}\cdot\left(\dfrac{ \text{node.value} }{ \text{node.visits} } + 1\right)$
                \If{node.parent.player $\neq$ node.player}
                    \State mean $\gets$ 1 $-$ mean
                \EndIf
            \Else
                \State mean $\gets$ 0
            \EndIf
            \State \Return $\text{mean} + \text{node.prior} \cdot C \cdot \dfrac{\sqrt{\text{node.parent.visits}}}{1 + \text{node.visits}}$  
        \EndFunction
    \end{algorithmic}
    \caption{Select Function for the Monte-Carlo Tree Search Algorithm}
    \label{alg:select}
\end{algorithm}
\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{search}{state, model, simulations}
            \State root $\gets$ Node(state)
            \ForAll{$n \in \{1, 2,3, \ldots, \text{simulations}\}$}
                \State node $\gets$ root
        
                \Call{select}{node}
        
                \State optional\_terminal\_value $\gets$ game.terminal\_value(node.state, node.action)
                
                \If{optional\_terminal\_value.has\_value()}
                    \State value $\gets$ optional\_terminal\_value.value()
                \Else
                    \State value $\gets$ \Call{expand}{node, model}
                \EndIf
        
                \State \Call{backpropagate}{node, value}
            \EndFor

            \State probs $\gets$ zero\_array(game.action\_size)
            \ForAll{i, child $\in$ root.children}
                \State probs[i] = child.visits
            \EndFor
            \State \Return probs / probs.sum()
        \EndFunction
    \end{algorithmic}
    \caption{Monte-Carlo Tree Search Algorithm}
    \label{alg:mcts}
\end{algorithm}

The algorithm repeats the selection phase until it encounters a node that is not expanded. There are two cases, it encounters terminal node or a non-terminal node. 

If it encounters a non-terminal node, then this signifies an action that has not been explored yet. The canonical MCTS uses a rollout phase that randomly selects an action from the legal actions, but similar to the alphazero paper we use the model to evaluate the possible actions and assign a probability to each of them. To restrict the model from assigning probabilities to invalid actions we obtain the legal actions from the game and filter the policy vector for only the legal actions.

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{expand}{parent, model}
            \State legal\_actions $\gets$ game.legal\_actions(parent.state)
            \State policy, wdl $\gets$ model(game.encode\_state(parent.state))
            \State policy $\gets$ \Call{softmax}{policy}
            \State policy $\gets$ \Call{filter}{legal\_actions, policy}
            \State policy $\gets$ policy / policy.sum()
        
            \For{$i \in \{0, ..., \text{legal\_actions.size}\}$}
                \State legal\_actions $\gets$ legal\_actions[i]
                \State child\_state $\gets$ game.apply\_action(parent.state, action)
                \State prior $\gets$ policy[i]
                \State parent.children $\gets$ parent.children $\cup$ (Node(child\_state, action, prior))
            \EndFor
            \State $p_w, p_d, p_l \gets wdl$ 
            \State \Return $p_w - p_l$
        \EndFunction
    \end{algorithmic}
    \caption{Expand Function for the Monte-Carlo Tree Search Algorithm}
    \label{alg:expand}
\end{algorithm}

If it encounters a terminal node, then it backpropagates the outcome of the game which is a scalar value $\in [-1, 1]$ which is $1$ for win, $-1$ for lose and $0$ for draw. The backpropagation phase is essential for updating the statistics of the nodes that have been traveresed by the current iteration. Here, it updates the cumulative value of each node as well as their respective visit counts.

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{backpropagate}{node, value}
            \While{node $\neq$ null}
                \State node.visits += 1
                \If{node.parent.state.player == node.state.player} 
                    \State node.value += value
                \Else
                    \State node.value -= value
                \EndIf
                \State node $\gets$ node.parent
            \EndWhile
        \EndFunction
    \end{algorithmic}
    \caption{Backpropagrate Function for the Monte-Carlo Tree Search Algorithm}
    \label{alg:backpropagate}
\end{algorithm}


\subsection{Self-Play}

The model is trained using the policy and WDL vectors obtained from the MCTS search, the data needed for this is obtained from self-play generation. In this process, the model produces a dataset comprised of a tuple of policy, outcome and state, which is then used to train the model.

\begin{algorithm}[htb]
    \begin{algorithmic}[1]
        \Function{self\_play}{model}
            \State history $\gets$ \{\}
            \State state $\gets$ game.initial\_state()
            \Loop
                \State policy $\gets$ \Call{search}{state, model}
                \State history $\gets$ history $\cup$ (state, probs)
                \State action $\gets$ \Call{sample\_distribution}{policy}
                \State new\_state $\gets$ game.apply\_action(state, action)
                \State outcome $\gets$ game.get\_outcome(new\_state)
                \If{game ended}
                    \State memory $\gets$ \{\}
                    \ForAll{(hist\_state, hist\_probs) $\in$ history}
                        \State outcome $\gets$ hist\_state.player == player ? outcome : outcome.flip()
                        \State memory $\gets$ memory $\cup$ (policy, outcome, state))
                    \EndFor
                    \State \Return memory
                \EndIf
                \State state $\gets$ new\_state
            \EndLoop
        \EndFunction
    \end{algorithmic}
    \caption{Pseudocode for the Self-Play Data Generation Phase of the AlphaZero Framework}
    \label{alg:data-generation}
\end{algorithm}

\subsection{Training}

The model is then trained using the data generated from the \ref{alg:data-generation}. The dataset is comprised of a tuples of the form $(\pi, v, s)$, where $\pi$ is the computed policy by MCTS, $v$ is a one-hot encoding of the outcome of the game, $[1, 0, 0]$ for a win, $[0, 1, 0]$ for a draw and $[0, 0, 1]$ for a loss and $s$ is the state of the game.

The loss $l$ is computed by the following equation,

\begin{align}
  \pi', v' &= f_{\theta}(s) \\
  l &= \sum_{}^{} v_{i}\log(v_{i}') + \sum_{}^{}\pi_{i}\log(\pi_{i}')
  \label{eq:loss}
\end{align}

The computed loss is then used by the optimizer for optimizing the parameters of the model. We use the Adam Optimizer \cite{kingma2017adammethodstochasticoptimization} as the optimizer. The process of generating self-play data followed by training is then repeated $n$ times, in each iteration the model is refined to output a similar probabilites and value generated by the MCTS algorithm.
